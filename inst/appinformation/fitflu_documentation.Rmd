---
title: Flu Fit Documentation
output:
  html_document:
    theme: null
    highlight: null
    fig_retina: null
    fig_caption: true
    mathjax: default 
    keep_md: false
bibliography: dsaide_references.bib
---

```{r, include = FALSE}
#*************************************
#general setup to define package and get path locations
#all paths are inside the package and retrieved with system.file
packagename = "DSAIDE"
helperdir = "helperfunctions"
mbmodeldir = "mbmodels"
figuredir = "media"
appdocdir = "appinformation" 
#*************************************
#Note: for this to process/knit, several helper functions need to be available (sourced) first
#those are in the inst/helperfunctions folder
#Note: in general, the "processing-script.R" in the docsfordevelopers should be used to produce the html docs
#manual knitting of each doc only during development/testing
#*************************************
files_to_source = list.files(system.file(helperdir,package = packagename),full.names=TRUE)
sapply(files_to_source, source) #sourcing needs to happen inside each Rmd file since knitr starts a new environment
#load the settings file for the current app 
#so we can automatically include figure, list the functions in the further information section
#and use other information specific to the current app for the task table generation
currentrmdfile = knitr::current_input() 
appsettings = get_settings(currentrmdfile,appdocdir,packagename)
```



## Overview {#shinytab1}
This app illustrates how to fit an SIR-type model to data and how to use simulated data to evaluate if it is possible to fit a specific model.



### Learning Objectives

* Know a basic approach to fitting SIR-type models to data
* Understand how to match data to model variables
* Start to appreciate the technical difficulties that can arise when fitting


## The Model {#shinytab2}

### Data
For this app, weekly mortality data from the 1918 influenza pandemic in New York City is used. The data comes from [@mills04]. You can read a bit more about the data by looking at its help file with `help('flu1918data')`. 

The data is reported in new deaths per week per 100,000 individuals. Our model (see next section) tracks _cumulative_, not _new_ deaths. The easiest way to match the two is to add up the weekly reported deaths in the data and compute cumulative deaths for each week. We can then fit that quantity directly to the model variable _D_. Adjustment for population size is also needed, which is done by dividing the reported death rate by 100,000 and multiplying with the population size. This is further discussed in the tasks. 

Alternatively, the model itself can be used to generate artificial data. We can then fit the model to this model-generated data. This is useful for diagnostic purposes, as you will learn by going through the tasks for this app.


### Simulation Model 
The underlying model that is being fit is a version of the basic SIR model. Since the available data is mortality, we need to keep track of dead individuals in the model, too. This can be achieved by including an additional compartment and letting a fraction of infected individuals move into the dead instead of the recovered compartment.


```{r modeldiagram,  fig.cap='Model diagram.',  echo=FALSE, out.width = "70%"}
knitr::include_graphics(here::here('inst/media',appsettings$modelfigname))
```


The equations for the model are given by

$$
\begin{aligned}
\dot S & = -bSI \\
\dot I & = bSI - gI \\
\dot R & = (1-f)gI \\
\dot D & = fgI
\end{aligned}
$$

Since the individuals in the _R_ compartment are not tracked in the data and do not further influence the model dynamics, we can ignore them here and can implement the model without the _R_ compartment, i.e., the simulation runs these equations. 

$$
\begin{aligned}
\dot S & = -bSI \\
\dot I & = bSI - gI \\
\dot D & = fgI
\end{aligned}
$$



### Model Fitting 

The app fits the model by minimizing the sum of square residuals (SSR) between model predictions for cumulative deaths and the cumulative number of reported deaths for all data points, i.e.

$$
SSR= \sum_t (D_t - D^{data}_t)^2
$$
where the sum runs over the times at which data was reported. 

It is also possible to set the app to fit the difference between the logarithm of data and model, i.e.
$$
SSR= \sum_t (\log(D_t) - \log(D^{data}_t))^2
$$

The choice to fit the data or the log of the data depends on the setting. Sometimes one approach is more suitable than the other. In this case, both approaches might be considered reasonable. The choice is a scientific one.

The app reports the final SSR for the fit. This is the lowest number (smallest discrepancy) between the data and the model predictions that the fitting routine was able to achieve. 

While minimizing the sum of square difference between data and model prediction is a very common approach, it is not the only one. A more flexible formulation of the problem is to define a likelihood function, which is a mathematical object that compares the difference between model and data based on assumption about the processes that might have led to the observed data. The likelihood has its maximum for the model settings that most closely describe the data. Under certain assumptions, maximizing the likelihood and minimizing the sum of squares are the same problem. Many modern approaches, both frequentist and bayesian, use the likelihood. So if you want to learn more about fitting SIR-type models to data, learning more about the likelihood and approaches based in it is a good idea. However, this goes beyond the goals of this app (and all current data fitting related apps included in DSAIDE). Interested readers are recommended to look further into this topic, I provided a few pointers in the resources section.


### Computer routines for fitting 

A computer routine does the minimization of the sum of squares. Many such routines, generally referred to as _optimizers_, exist. For simple problems, e.g., fitting a linear regression model to data, any of the standard routines work fine. For the kind of minimization problem we face here, which involves a differential equation, it often makes a difference what numerical optimizer routine one uses. `R` has several packages for that purpose. In this app, we make use of the optimizer algorithms called _COBYLA_, _Nelder-Mead_ and _Subplex_ from the the `nloptr` package. This package provides access to a large number of optimizers and is a good choice for many optimization/fitting tasks. For more information , see the help files for the `nloptr` package and especially the [nlopt website](https://nlopt.readthedocs.io/).

For any problem that involves fitting ODE models to data, it is often important to try different numerical routines and different starting points to ensure results are consistent. This will be discussed a bit in the tasks.

Another feature that is often good to have is the ability to specify lower and upper bounds for parameters. In theory, if the model is a decent approximation of the underlying real system, then the best fit of the model should happen for biologically reasonable parameter values and _in theory_ providing bounds is not necessary. However, _in practice_, having bounds is very useful. First, even if the fitting routine would eventually end up with good parameter estimates, it might on the way to getting there try unreasonable values. For instance most of our parameters need to be positive, sometimes they need to be between 0 and 1, and sometimes combinations of parameters can't be crazy (e.g. we don't want a combination of transmission rate and recovery rate that would lead to an unrealistically large reproductive number). With any of those unreasonable choices, the code running the differential equation model might 'blow up' and the fitting might fail. Providing bounds for parameters solves that problem. Another issue might arise that for the best fit, one of the parameters takes on the value of the lower or upper bound. This means if you had chosen the bounds wider, you might get an even better fit. You can try adjusting bounds. At some point the bounds might get unreasonable. If you run into this situation, that you are trying to fit a model and the best fit happens for parameter values that are at the bounds or biologically unreasonable, it either means you are trying to estimate more parameters than your data allows you to, or

A side note: Using a Bayesian approach is an alternative to providing constraints for parameters, and it is in some sense a more disciplined way. The problem is that fitting differential equation models (or stochastic equivalents) in a Bayesian framework is still computationally very expensive and often would take too long to be a reasonable option. Though fitting software is getting more powerful and thus it will become more and more feasible.



## What to do {#shinytab3}

Since the data is in weeks, we also run the model in units of weeks.



```{r, echo=FALSE, eval=TRUE}

#this is the running counter for the records which starts at 1 
rc=1

#empty object, will hold all outcomes
alloutcomes = NULL

#########################
# Task 1
#########################
tid = 1
tasktext = "The population of New York City in 1918 was roughly around 5E6 individuals. Use that number as your initial starting value for the susceptibles. Set infected individuals to 1, no initial dead. The model parameters, _b_, _g_, and _f_, are being fit. Even though they are being fit/estimated, we need to provide starting values for the fitting routine. Set the starting value for the infection rate to _b=1e-6_, set the initial recovery rate such that it corresponds to an infectious period of one week, and start with the assumption that one percent of individuals died.

For each fitted parameter, choose some lower and upper bounds. Note that if the lower bound is not lower/equal and the upper, not higher/equal than the starting value of the parameter, you will get an error message when you try to run the model. For now, we ignore the option to generate simulated data. Therefore set usesimdata to 0 and the parameter values for simulated data can be arbitrary, they will be ignored. The noise parameter will also be ignored. We'll fit the data 'as is' (not the log-transformed data), so set logfit to 0. Start with a maximum of 1 fitting step/iteration and solver type 1. Run the simulation. Since you only do a single iteration, nothing is optimized. We are just doing this so you can see the time-series produced with these starting conditions. The 'best fit' parameter values are the same you started with. To that end, with that choice, your SSR should be 7.3E12."
nrec = 1 # number of items to record
out_records = c("Nothing")
out_types = rep("None",nrec)
out_notes = c("")
outcomes = data.frame( TaskID = rep(tid,nrec),
                       TaskText = rep(tasktext,nrec),
                      RecordID = paste0('T',tid,'R',(1:nrec)),
                      Record = out_records, 
                      Type = out_types, 
                      Note = out_notes)
alloutcomes = rbind(alloutcomes,outcomes)
rc = rc + nrec #increment record counter by number of outcomes to record for this task 

#########################
# Task 2
#########################
tid = tid + 1
tasktext = "Set the maximum number of iterations to 50, and re-run the simulation. All of the other settings should be the same from task 1. Look at the results. The plot shows the final fit. The model-predicted curve for deaths will be closer to the data. Also, the SSR value should have gone down, to 5.6E11, indicating a better fit. Also printed below the figure are the values of the fitted parameters at the end of the fitting process. Set the iterations to 100, and re-run the simulation. You should see further improvement in SSR, to 3.6E11. That indicates the previous fit was not the best fit. (The best fit is the one with the lowest possible SSR)."
nrec = 2 # number of items to record
out_records = c("Estimate for parameter f (fraction dying) after 50 iterations.",
            "Estimate for parameter f (fraction dying) after 100 iterations.")
out_types = rep("Numeric",nrec)
out_notes = rep("Round to two significant digits, report as non-scientic notation (as X.YZ)",nrec)
outcomes = data.frame( TaskID = rep(tid,nrec),
                       TaskText = rep(tasktext,nrec),
                      RecordID = paste0('T',tid,'R',(1:nrec)),
                      Record = out_records, 
                      Type = out_types, 
                      Note = out_notes)
alloutcomes = rbind(alloutcomes,outcomes)
rc = rc + nrec #increment record counter by number of outcomes to record for this task 


#########################
# Task 3
#########################
tid = tid + 1
tasktext = "Repeat the fits for 1, 50 and 100 iterations using solver/optimizer number 2, then number 3. Keep all other settings as before. You will notice that the different solvers do not give the same results. The reason for that is that different solvers are improving fits at different rates. Unfortunately, there is not always a single best optimizer. It depends on the problem/system. Thus it's usually good to try multiple optimizers. Depending on the speed of your computer, you can try to increase the number of iterations and see what best fit (smallest SSR) you can achieve. 

Generally, with increasing iterations, the fits get better. A fitting step or iteration is essentially an attempt by the underlying code to find the best possible model. Increasing the tries usually improves the fit. In practice, one should not specify a fixed number of iterations. We do it here, so things run reasonably fast. Instead, one should ask the solver to run as long as it takes until it can't find a way to improve the fit (can't further reduce the SSR). The technical expression for this is that the solver has converged to the solution. This can be done with the solver used here (`nloptr` R package), but it would take too long, so we implement a hard stop after the specified number of iterations."

nrec = 2 # number of items to record
out_records = c("Estimate for parameter f (fraction dying) after 100 iterations, solver 2.",
            "Estimate for parameter f (fraction dying) after 100 iterations, solver 3.")
out_types = rep("Numeric",nrec)
out_notes = rep("Round to three significant digits, report as non-scientic notation (as N.XYZ)",nrec)
outcomes = data.frame( TaskID = rep(tid,nrec),
                       TaskText = rep(tasktext,nrec),
                      RecordID = paste0('T',tid,'R',(1:nrec)),
                      Record = out_records, 
                      Type = out_types, 
                      Note = out_notes)
alloutcomes = rbind(alloutcomes,outcomes)
rc = rc + nrec #increment record counter by number of outcomes to record for this task 



# #########################
# # Task 3
# #########################
# tid = tid + 1
# tasktext = "Ideally, with enough iterations, all solvers should reach the best fit with the lowest possible SSR. In practice, that does not always happen. Often it depends on the starting conditions. Let's explore whether starting values matter. Set everything as in task 1. Run the simulation for a single iteration, you should get an SSR=7.3E12, and the best fit parameter values should equal the starting values, just as in task 1. Now run the simulation using solver 3 and 200 iterations. If everything was set correctly, you should get SSR=4.3E10. Also, note the best-fit parameter values." 
# 
# 
# * Now change the starting values to a 5 percent mortality fraction and half a week of infectiousness duration. Re-run the simulation at 1 and 200 iterations. You should find before/after SSR values of 4.7E12 and 1.8E11. This means the starting values were somewhat better (7.3E12 versus 4.7E12), but it didn't help much for the fitting, at least not for 200 steps (4.3E10 versus 1.8E11). 
# * By trying different starting values, solvers, and number of iterations you can get an idea of the influence starting conditions can have on fitting performance and results.
# 
# In general, picking good starting values is essential. One can get them by trying an initial visual fit or by doing several short fits, and use the best fit values at the end as starting values for a new fit.
# Especially if you want to fit multiple parameters, optimizers can 'get stuck'. If they get stuck, even running them for a long time might not find the best fit. One way an optimizer can get stuck is when a solver finds a local optimum. The local optimum is a good fit, and now as the solver varies parameters, each new fit is worse, so the solver "thinks" it found the best fit, even though there are better ones further away in parameter space. Many solvers - even so-called 'global' solvers - can get stuck. Unfortunately, we never know if the solution is real or if the solver is stuck in a local optimum. One way to figure this out is to try different solvers and different starting conditions, and let each one run for a long time. If all return the same answer, no matter what type of solver you use and where you start, it's quite likely (though not guaranteed) that we found the overall best fit (lowest SSR).
# 
# The problem of 'getting stuck' is something that frequently happens when trying to fit ODE models, which is in contrast to fitting with more standard models (e.g., a linear regression model), where it is not a problem. The technical reason for this is that a simple regression optimization is _convex_ while fitting an ODE model is usually not. That's why you don't have to worry if you found the right solution if you use the `lm` or `glm` functions for fitting in `R`. When fitting more complicated models such as ODE or similar models, you do have to carefully check that the "best fit" is not the result of a local optimum.
# 
# **Record**
# 
# * Best-fit value for b (infection rate) using solver 3 and 200 iterations
# * Best-fit value for g (recovery rate) using solver 3 and 200 iterations
# * Best-fit value for f (fraction dying) using solver 3 and 200 iterations
# 
# ### Task 5 
# * You might have noticed that the model is furthest away from the data during the first several weeks. One reason could be that we started with 1 infected person. In reality, having a single infected is not likely; in fact, there were probably already a lot of infected patients, and as a result, deaths began to accumulate faster. I could have written the model so you could fit the initial infected. I didn't since it would overcomplicate the example and likely leading to overfitting. We don't have much data, and if we fit too many parameters, the model would likely overfit the data (more on that below). Instead, let's see if we can get a better fit by manually adjusting the initial number of infected.
# * Set values back to those of task 1 (the 'Reset Inputs' button should do the trick, you will still need to change the number of iterations to 1). Do a quick run with a single iteration to make sure you get the SSR from task 1.
# * Now set the initial number of infected to 1000, re-run a single iteration. You'll notice in the plot that the model predicted deaths moves closer to the data in the first few weeks. 
# * Maybe surprisingly, the SSR does not shrink. Can you figure out why? Take another look at the SSR equation and think about the impact of the later (higher value) data points compared to the earlier ones for the total SSR.
# 
# **Record**
# 
# * Nothing
# 
# ### Task 6 
# In the previous task, you learned that for the SSR computation, differences between large data and model values - which themselves tend to be larger in magnitude - often dominate the SSR expression. As an extreme example, if you have 2 data points you fit, one at 1E10 and the model predicts 1.1E10, that's a difference of 1E9. The second data point is 1E7, and the model predicts 1E6. This is in some sense a more significant discrepancy, but the difference is only 9E6, much smaller than the 1E9 for the first data point.
# 
# One way to give data of different magnitude more comparable weights is by fitting the log of the data. Note that fitting the data or the log of the data are different, and the choice should be made based on scientific/biological rationale.
# 
# * Let's take a look at fitting the log. Set everything as in task 1, set fitlog to 1, and do a single iteration. The plot should look the same as in task 1, but the SSR is now computed on the log of the data and the model and should be 40.72. 
# * Now increase the number of initial infected to 1000 again. You'll see the model predictions go closer to the initial data again, and the SSR now becomes lower.
# * Repeat some of the task 1-3 exercises, now fitting the log of the data. Compare and contrast how results do and do not change.
# 
# Again, fitting either a linear or log scale are both reasonable approaches, and the choice should be made based on the underlying biology/science. A good rule of thumb is that if the data spans several orders of magnitude, fitting on the log scale is probably the better option. You probably already realized that you can of course, not compare the SSR between the two fitting approaches. SSR values only make sense to compare once you determined the scale for fitting.
# 
# **Record**
# 
# The SSR with 1000 infected 
# 
# ### Task 7
# One consideration when fitting these kinds of mechanistic models to data is the balance between data availability and model complexity. The more and "richer" the data available the more parameters you can estimate and therefore, the more detailed a model can be. If you 'ask too much' from the data, it leads to the problem of overfitting. Overfitting can be thought of as trying to estimate more parameters than can be robustly estimated for a given dataset. One way to safeguard against overfitting is by probing if the model can in generate estimates in a scenario close to known values for that parameter. To do so, we can use our model with specific parameter values and simulate data. We can then fit the model to this simulated data. If everything works, we expect that - ideally independent of the starting values for our solver - we end up with estimated best-fit parameter values that agree with the ones we used to simulate the artificial data. We'll try this now with the app.
# 
# * Set everything as in task 1. Now set the parameter values _bsim_, _gsim_, and _fsim_ to the same values as the values used for starting the fitting routine (_b_, _g_, _f_).
# * Set _usesimdata_ to 1. 
# * Run for 1 fitting step. You should now see that the data has changed. Instead of the real data, we now use simulated data. Since the parameter values for the simulated data and the starting values for the fitting routine are the same, the time-series is on top of the data, and the SSR is (up to rounding errors) 0. 
# 
# **Record**
# 
# * Nothing
# 
# ### Task 8 
# Let's see if the fitting routine can recover parameters from a simulation if we start with different initial/starting values.
# 
# * Choose as values for simulated data parameters _bsim=5E-7_, _gsim=0.5_ and _fsim = 0.01_.
# * Everything else should be as in task 1. Importantly, the starting values for the parameters _b_ and _g_ are now different than the values used for the simulation.
# * Fit to the simulated data, run for 1 iteration. You'll see the (simulated) data change again. The SSR should be 7.6E9 (assuming we fit on a linear scale).
# * If you now run the fitting for many iterations/steps, what do you expect the final fit values for the parameters and the SSR to be?
# * Test your expectation by running for 100+ fitting steps with the different solvers. 
# 
# **Record**
# 
# * Nothing
# 
# ### Task 9 
# Theory suggests that if we run enough iterations, we should obtain a best fit with an SSR close to 0 and best fit values that agree with those used to generate the artificial data. You might find that this does not happen for all 3 solvers within a reasonable number of iterations. For instance, using solver 2 and 1000 iterations should get you pretty close to what you started with.
# 
# That indicates that you can potentially estimate these parameters with that kind of data, at least if there is no noise. This is the most basic test. If you can't get the best fit values to be the same as the ones you used to make the data, it means you are trying to fit more parameters than your data can support, i.e., you are overfitting. At that point, you will have to either get more data or reduce your fitted parameters. Reducing fitted parameters can be done by either fixing some parameters based on biological a priori knowledge or by reducing the number of parameters through model simplification.
# 
# **Record**
# 
# * Nothing
# 
# ### Task 10 
# * Play around with different values for the parameters used to generate artificial data, and different values for the starting conditions and see if you find scenarios where you might not be able to get the solver to converge to a solution that agrees with the one you started with.
# * Also explore what if any impact fitting the data on a linear versus log scale has. 
# 
# **Record**
# 
# * Nothing
# 
# ### Task 11 
# * To make things a bit more realistic and harder, one can also add noise on top of the simulated data. Try that by playing with the 'noise added' parameter and see how well you can recover the parameter values for the simulation. Start with a small amount (e.g., 0.01) and increase.
# 
# Note that since you now change your data after you simulated it, you don't expect the parameter values for the simulation and those you obtain from your best fit to be the same. However, if the noise is not too large, you expect them to be similar.
# 
# You will likely find that for certain combinations of simulated data, noise added, and specific starting conditions, you might not get estimates that are close to those you used to create the data. This suggests that even for this simple model with 3 parameters, estimating those 3 parameters based on the available data is not straightforward.
# 
# **Record**
# 
# * Nothing
# 
# ### Task 12
# * Keep exploring. Fitting these kinds of models can be tricky at times, and you might find strange behavior in this app that you don't expect. Try to get to the bottom of what might be going on. This is an open-ended exploration, so I can't give you a "hint". Just try different things, try to understand as much as possible of what you observe.
# 
# **Record**
# 
# * Nothing
```



```{r echo=FALSE}
#save the fully filled task table to a tsv file
alloutcomes$QuizID = paste0('dsaide_',appsettings$appid)
alloutcomes$AppTitle = appsettings$apptitle
alloutcomes$AppID = appsettings$appid
alloutcomes$Answer = "" #add empty answer column, just for consistency. Should probably be removed here.
write.table(alloutcomes, paste0(appsettings$appid,"_tasktable.tsv"), append = FALSE, sep = "\t", row.names = F, col.names = TRUE)
```


```{r, echo=FALSE, results='asis'}
# Take all the text stored in the table and print the tasks and items to record
write_tasktext_new(alloutcomes)
```



## Further Information {#shinytab4}
* This app (and all others) are structured such that the Shiny part (the graphical interface you see and the server-side function that goes with it) calls an underlying R script (or several) which runs the simulation for the model of interest and returns the results.
* For this app, the underlying function running the simulation is called ``r appsettings$simfunction``. You can call them directly, without going through the shiny app. Use the `help()` command for more information on how to use the functions directly. If you go that route, you need to use the results returned from this function and produce useful output (such as a plot) yourself. 
* You can also download all simulator functions and modify them for your own purposes.  Of course, to modify these functions, you will need to do some coding.
* For examples on using the simulators directly and how to modify them, read the package vignette by typing `vignette('DSAIDE')` into the R console.
* A good source for fitting models in `R` is [@bolker08]. Note though that the focus is on ecological data and ODE-type models are not/barely discussed.
* This book [@hilborn97] has nice explanations of data fitting, model comparison, etc. but is more theoretical.
* Many good online material exists on fitting/inference. Most of the material is explained in the context of static, non-mechanistic, statistical or machine learning models, but a lot of the principles apply equally to ODEs.
* A discussion of overfitting (also called 'identifiability problem') for ODEs is [@miao11a].
* Advanced functionality to fit stochastic models can be found in the `pomp` package in R. (If you do not know what stochastic models are, check out the stochastic apps in DSAIDE.)
* The data for this study is saved in the data variable `flu1918data`, you can read more about it by looking at its help file entry `help(flu1918data)`. The publication from which the data comes is [@mills04].

### References


