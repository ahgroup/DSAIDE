"QuizID"	"AppID"	"AppTitle"	"TaskID"	"TaskText"	"RecordID"	"Record"	"Type"	"Note"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	1	"Start with the pre-set input values. Since sampling of parameters involves uncertainty/randomness, we need to use random numbers. We still want results to be reproducible. That's where the random number seed comes in. As long as the seed is the same, the code should produce the same (pseudo)-random numbers each time, thus ensuring reproducibility. Let's explore this. Leave all settings as they are, run the simulation twice with the same random number seed (value of 100), check to make sure you get exactly the same result twice. Now change the random number seed to 102, run again. You should see the results changed. 

If you have more samples, the results generally become more robust to changes in the underlying sample generation (determined by the random number seed). Try checking this by repeating simulations for the two random seeds, but now run 50 samples for each seed (this may take a while, depending on the speed of your computer).

Pay attention to variability in the different quantities reported below the plot. You should see less variability in the central quantities (mean, median) for the larger sample size, extremes (min/max) will continue to fluctuate more.

In general, the amount by which you change the random seed doesn't matter, the impact it has on outcomes is random. It just so happens that 101 gives a maybe confusing result for 50 samples - try if you want.  

Note that each sample means one simulation of the underlying dynamical model, so as sample numbers increase, things slow down. Also note the _system might not have reached steady state_ message. We are ignoring it for now, but if for too many of the samples steady state has not been reached, the results for _S~final~_ and _I~final~_ do not reflect steady-state values. Increasing the simulation time can help the system reach a steady state (if there is one). For some parameter combinations, that can take very long."	"T1R1"	"Mean number of peak infected, samples = 5, rngseed = 100"	"Rounded_Integer"	"Report the rounded integer"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	1	"Start with the pre-set input values. Since sampling of parameters involves uncertainty/randomness, we need to use random numbers. We still want results to be reproducible. That's where the random number seed comes in. As long as the seed is the same, the code should produce the same (pseudo)-random numbers each time, thus ensuring reproducibility. Let's explore this. Leave all settings as they are, run the simulation twice with the same random number seed (value of 100), check to make sure you get exactly the same result twice. Now change the random number seed to 102, run again. You should see the results changed. 

If you have more samples, the results generally become more robust to changes in the underlying sample generation (determined by the random number seed). Try checking this by repeating simulations for the two random seeds, but now run 50 samples for each seed (this may take a while, depending on the speed of your computer).

Pay attention to variability in the different quantities reported below the plot. You should see less variability in the central quantities (mean, median) for the larger sample size, extremes (min/max) will continue to fluctuate more.

In general, the amount by which you change the random seed doesn't matter, the impact it has on outcomes is random. It just so happens that 101 gives a maybe confusing result for 50 samples - try if you want.  

Note that each sample means one simulation of the underlying dynamical model, so as sample numbers increase, things slow down. Also note the _system might not have reached steady state_ message. We are ignoring it for now, but if for too many of the samples steady state has not been reached, the results for _S~final~_ and _I~final~_ do not reflect steady-state values. Increasing the simulation time can help the system reach a steady state (if there is one). For some parameter combinations, that can take very long."	"T1R2"	"Mean number of peak infected, samples = 5, rngseed = 102"	"Rounded_Integer"	"Report the rounded integer"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	1	"Start with the pre-set input values. Since sampling of parameters involves uncertainty/randomness, we need to use random numbers. We still want results to be reproducible. That's where the random number seed comes in. As long as the seed is the same, the code should produce the same (pseudo)-random numbers each time, thus ensuring reproducibility. Let's explore this. Leave all settings as they are, run the simulation twice with the same random number seed (value of 100), check to make sure you get exactly the same result twice. Now change the random number seed to 102, run again. You should see the results changed. 

If you have more samples, the results generally become more robust to changes in the underlying sample generation (determined by the random number seed). Try checking this by repeating simulations for the two random seeds, but now run 50 samples for each seed (this may take a while, depending on the speed of your computer).

Pay attention to variability in the different quantities reported below the plot. You should see less variability in the central quantities (mean, median) for the larger sample size, extremes (min/max) will continue to fluctuate more.

In general, the amount by which you change the random seed doesn't matter, the impact it has on outcomes is random. It just so happens that 101 gives a maybe confusing result for 50 samples - try if you want.  

Note that each sample means one simulation of the underlying dynamical model, so as sample numbers increase, things slow down. Also note the _system might not have reached steady state_ message. We are ignoring it for now, but if for too many of the samples steady state has not been reached, the results for _S~final~_ and _I~final~_ do not reflect steady-state values. Increasing the simulation time can help the system reach a steady state (if there is one). For some parameter combinations, that can take very long."	"T1R3"	"Mean number of peak infected, samples = 50, rngseed = 100"	"Rounded_Integer"	"Report the rounded integer"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	1	"Start with the pre-set input values. Since sampling of parameters involves uncertainty/randomness, we need to use random numbers. We still want results to be reproducible. That's where the random number seed comes in. As long as the seed is the same, the code should produce the same (pseudo)-random numbers each time, thus ensuring reproducibility. Let's explore this. Leave all settings as they are, run the simulation twice with the same random number seed (value of 100), check to make sure you get exactly the same result twice. Now change the random number seed to 102, run again. You should see the results changed. 

If you have more samples, the results generally become more robust to changes in the underlying sample generation (determined by the random number seed). Try checking this by repeating simulations for the two random seeds, but now run 50 samples for each seed (this may take a while, depending on the speed of your computer).

Pay attention to variability in the different quantities reported below the plot. You should see less variability in the central quantities (mean, median) for the larger sample size, extremes (min/max) will continue to fluctuate more.

In general, the amount by which you change the random seed doesn't matter, the impact it has on outcomes is random. It just so happens that 101 gives a maybe confusing result for 50 samples - try if you want.  

Note that each sample means one simulation of the underlying dynamical model, so as sample numbers increase, things slow down. Also note the _system might not have reached steady state_ message. We are ignoring it for now, but if for too many of the samples steady state has not been reached, the results for _S~final~_ and _I~final~_ do not reflect steady-state values. Increasing the simulation time can help the system reach a steady state (if there is one). For some parameter combinations, that can take very long."	"T1R4"	"Mean number of peak infected, samples = 50, rngseed = 102"	"Rounded_Integer"	"Report the rounded integer"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	2	"Recall the underlying SIR model and its behavior. If you don't remember, revisit the apps mentioned in the overview tab that discuss this model. Use your understanding of the model to predict what happens to the plots and the outcomes reported under the plots if you increase both lower and upper bound for the infection rate. Increase lower/upper bounds by a factor of 2. This means you are now sampling b in the range of 0.01 - 0.02. From previous apps, you know that this also implies that you increased R0. Thus, we expect larger outbreaks. Use 50 samples and rngseed=100. Run the simulation with the new ranges of b, see how results change. Now go the opposite way and lower the initial lower/upper bounds by a factor of 2, such that the range is 0.0025 - 0.005. Run the simulation, see how results change. As expected, a lower transmission rate and thus a lower R0 leads to smaller outbreaks."	"T2R1"	"Mean number of peak infected, increased transmission"	"Rounded_Integer"	"Report the rounded integer"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	2	"Recall the underlying SIR model and its behavior. If you don't remember, revisit the apps mentioned in the overview tab that discuss this model. Use your understanding of the model to predict what happens to the plots and the outcomes reported under the plots if you increase both lower and upper bound for the infection rate. Increase lower/upper bounds by a factor of 2. This means you are now sampling b in the range of 0.01 - 0.02. From previous apps, you know that this also implies that you increased R0. Thus, we expect larger outbreaks. Use 50 samples and rngseed=100. Run the simulation with the new ranges of b, see how results change. Now go the opposite way and lower the initial lower/upper bounds by a factor of 2, such that the range is 0.0025 - 0.005. Run the simulation, see how results change. As expected, a lower transmission rate and thus a lower R0 leads to smaller outbreaks."	"T2R2"	"Mean number of peak infected, decreased transmission"	"Rounded_Integer"	"Report the rounded integer"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	3	"This model includes births, deaths and waning immunity. So far, they are turned off since both lower and upper bounds are set to 0. Now let's turn on births and deaths. The presence of births and deaths means the underlying time-series (which we can't see, but could look at in one of the earlier apps), produces an initial large outbreak, followed by a few smaller outbreaks and then settles down to a steady state. Reset everything to the initial values. Then set the range for the birth rate _n_ to 50 - 100, the range for death rate _m_ to 0.05 - 0.1. (If you are paying attention, you might have noticed that the range for the death rate means a natural life-span of 10 - 20 weeks. That's of course way too short for humans. It's chosen here to make the model settle down in a reasonable time-frame. You can think of maybe modeling an animal disease in a host that doesn't live very long.)

Let's explore how the steady state is impacted if we change birth and death rates. Run the model with the specified settings for 50 simulations with random seed at 100. Look at the values at final steady state (and record the one requested).

Now, if we increase births or deaths, which of the outcomes do you expect to change, and in which direction? Test your assumption by increasing the bounds for the birth rate such that the range is now 100 - 200. Re-run the simulation. Then set the birth rate back to the range 50-100 and now increase the bounds for the death rate such that the range is now 0.1 - 0.2. Repeat the simulations. Finally, have both birth and death rates at the increased ranges of 50-100 and 0.1-0.2 respectively, run simulations again. 

You should see that a change in birth rate has essentially no impact on the final steady state of S, while an increase in the death rate leads to an increase in _S~final~_. If you are not sure why you are getting these results, remember what you did in the _ID Patterns_ app (or revisit that app). There you found an equation for the steady state for _S_ for this type of model, given by S=(g + m)/b. This shows that changes in birth rate do not affect the steady state, while changes in death rate do."	"T3R1"	"Mean number of susceptible at end of simulation, baseline"	"Rounded_Integer"	"Report the rounded integer"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	3	"This model includes births, deaths and waning immunity. So far, they are turned off since both lower and upper bounds are set to 0. Now let's turn on births and deaths. The presence of births and deaths means the underlying time-series (which we can't see, but could look at in one of the earlier apps), produces an initial large outbreak, followed by a few smaller outbreaks and then settles down to a steady state. Reset everything to the initial values. Then set the range for the birth rate _n_ to 50 - 100, the range for death rate _m_ to 0.05 - 0.1. (If you are paying attention, you might have noticed that the range for the death rate means a natural life-span of 10 - 20 weeks. That's of course way too short for humans. It's chosen here to make the model settle down in a reasonable time-frame. You can think of maybe modeling an animal disease in a host that doesn't live very long.)

Let's explore how the steady state is impacted if we change birth and death rates. Run the model with the specified settings for 50 simulations with random seed at 100. Look at the values at final steady state (and record the one requested).

Now, if we increase births or deaths, which of the outcomes do you expect to change, and in which direction? Test your assumption by increasing the bounds for the birth rate such that the range is now 100 - 200. Re-run the simulation. Then set the birth rate back to the range 50-100 and now increase the bounds for the death rate such that the range is now 0.1 - 0.2. Repeat the simulations. Finally, have both birth and death rates at the increased ranges of 50-100 and 0.1-0.2 respectively, run simulations again. 

You should see that a change in birth rate has essentially no impact on the final steady state of S, while an increase in the death rate leads to an increase in _S~final~_. If you are not sure why you are getting these results, remember what you did in the _ID Patterns_ app (or revisit that app). There you found an equation for the steady state for _S_ for this type of model, given by S=(g + m)/b. This shows that changes in birth rate do not affect the steady state, while changes in death rate do."	"T3R2"	"Mean number of susceptible at end of simulation, increased birth"	"Rounded_Integer"	"Report the rounded integer"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	3	"This model includes births, deaths and waning immunity. So far, they are turned off since both lower and upper bounds are set to 0. Now let's turn on births and deaths. The presence of births and deaths means the underlying time-series (which we can't see, but could look at in one of the earlier apps), produces an initial large outbreak, followed by a few smaller outbreaks and then settles down to a steady state. Reset everything to the initial values. Then set the range for the birth rate _n_ to 50 - 100, the range for death rate _m_ to 0.05 - 0.1. (If you are paying attention, you might have noticed that the range for the death rate means a natural life-span of 10 - 20 weeks. That's of course way too short for humans. It's chosen here to make the model settle down in a reasonable time-frame. You can think of maybe modeling an animal disease in a host that doesn't live very long.)

Let's explore how the steady state is impacted if we change birth and death rates. Run the model with the specified settings for 50 simulations with random seed at 100. Look at the values at final steady state (and record the one requested).

Now, if we increase births or deaths, which of the outcomes do you expect to change, and in which direction? Test your assumption by increasing the bounds for the birth rate such that the range is now 100 - 200. Re-run the simulation. Then set the birth rate back to the range 50-100 and now increase the bounds for the death rate such that the range is now 0.1 - 0.2. Repeat the simulations. Finally, have both birth and death rates at the increased ranges of 50-100 and 0.1-0.2 respectively, run simulations again. 

You should see that a change in birth rate has essentially no impact on the final steady state of S, while an increase in the death rate leads to an increase in _S~final~_. If you are not sure why you are getting these results, remember what you did in the _ID Patterns_ app (or revisit that app). There you found an equation for the steady state for _S_ for this type of model, given by S=(g + m)/b. This shows that changes in birth rate do not affect the steady state, while changes in death rate do."	"T3R3"	"Mean number of susceptible at end of simulation, increased death"	"Rounded_Integer"	"Report the rounded integer"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	3	"This model includes births, deaths and waning immunity. So far, they are turned off since both lower and upper bounds are set to 0. Now let's turn on births and deaths. The presence of births and deaths means the underlying time-series (which we can't see, but could look at in one of the earlier apps), produces an initial large outbreak, followed by a few smaller outbreaks and then settles down to a steady state. Reset everything to the initial values. Then set the range for the birth rate _n_ to 50 - 100, the range for death rate _m_ to 0.05 - 0.1. (If you are paying attention, you might have noticed that the range for the death rate means a natural life-span of 10 - 20 weeks. That's of course way too short for humans. It's chosen here to make the model settle down in a reasonable time-frame. You can think of maybe modeling an animal disease in a host that doesn't live very long.)

Let's explore how the steady state is impacted if we change birth and death rates. Run the model with the specified settings for 50 simulations with random seed at 100. Look at the values at final steady state (and record the one requested).

Now, if we increase births or deaths, which of the outcomes do you expect to change, and in which direction? Test your assumption by increasing the bounds for the birth rate such that the range is now 100 - 200. Re-run the simulation. Then set the birth rate back to the range 50-100 and now increase the bounds for the death rate such that the range is now 0.1 - 0.2. Repeat the simulations. Finally, have both birth and death rates at the increased ranges of 50-100 and 0.1-0.2 respectively, run simulations again. 

You should see that a change in birth rate has essentially no impact on the final steady state of S, while an increase in the death rate leads to an increase in _S~final~_. If you are not sure why you are getting these results, remember what you did in the _ID Patterns_ app (or revisit that app). There you found an equation for the steady state for _S_ for this type of model, given by S=(g + m)/b. This shows that changes in birth rate do not affect the steady state, while changes in death rate do."	"T3R4"	"Mean number of susceptible at end of simulation, increased birth and death"	"Rounded_Integer"	"Report the rounded integer"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	4	"Let's continue with the exploration of the last task, to illustrate an important point. Set ranges for birth rate to 10 - 20 and death rate range 0.1 - 0.2. Run 50 simulations with random seed at 100. Look at the values at final steady state (and record the one requested).

Now increase the range for the birth rate to 20 - 40. Based on the last task, the final number of susceptible should not change. This is not what you will see, instead the value goes up. Can you figure out why this is the case? As a hint, it has to do with the reproductive number. 

Here is a detailed explanation: Remember (from earlier apps) that the reproductive number for this model is R~0~ = b * S~0~/(g+m). You also learned previously that for this kind of model, in the presence of births and deaths, the steady value for S~0~ is S~0~ = n/m. Combining those 2 equations, you get R~0~ = b * n/((g+m) * m). We can explore the range of possible R~0~ values by using the minimum and maximum of the ranges (we'll just use g=1 since the variation is fairly small). Plugging in numbers for the baseline gives a range for R~0~ from 0.005*10/((1+0.2)*0.2) to 0.01*20/((1+0.1)*0.1), which is rougly 0.2 to 1.8. Thus a good number of samples will have R~0~ < 1 and thus the steady state equation does not apply. As you increase the birth rate, more values move into the R~0~ > 1 range. The computed average mixes both the R~0~ > 1 samples, for which the steady state equation applies, and those with R0<1, for which that equation does not apply. Therefore, the overall results don't quite make sense. (You can check that in the previous task, parameter ranges were such that all R~0~ values were larger than 1.)

This is an important point. When you sample your parameters, you want to make sure that it doesn't put the model into completely different regimes, otherwise you are combining 2 very different situations and the combined results are hard to interpret. To guard against this, it is often good to sample ranges of parameters in pieces, instead of one very wide range. And it's also always good to explore the outcomes in a plot."	"T4R1"	"Mean number of susceptible at end of simulation, baseline"	"Rounded_Integer"	"Report the rounded integer"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	4	"Let's continue with the exploration of the last task, to illustrate an important point. Set ranges for birth rate to 10 - 20 and death rate range 0.1 - 0.2. Run 50 simulations with random seed at 100. Look at the values at final steady state (and record the one requested).

Now increase the range for the birth rate to 20 - 40. Based on the last task, the final number of susceptible should not change. This is not what you will see, instead the value goes up. Can you figure out why this is the case? As a hint, it has to do with the reproductive number. 

Here is a detailed explanation: Remember (from earlier apps) that the reproductive number for this model is R~0~ = b * S~0~/(g+m). You also learned previously that for this kind of model, in the presence of births and deaths, the steady value for S~0~ is S~0~ = n/m. Combining those 2 equations, you get R~0~ = b * n/((g+m) * m). We can explore the range of possible R~0~ values by using the minimum and maximum of the ranges (we'll just use g=1 since the variation is fairly small). Plugging in numbers for the baseline gives a range for R~0~ from 0.005*10/((1+0.2)*0.2) to 0.01*20/((1+0.1)*0.1), which is rougly 0.2 to 1.8. Thus a good number of samples will have R~0~ < 1 and thus the steady state equation does not apply. As you increase the birth rate, more values move into the R~0~ > 1 range. The computed average mixes both the R~0~ > 1 samples, for which the steady state equation applies, and those with R0<1, for which that equation does not apply. Therefore, the overall results don't quite make sense. (You can check that in the previous task, parameter ranges were such that all R~0~ values were larger than 1.)

This is an important point. When you sample your parameters, you want to make sure that it doesn't put the model into completely different regimes, otherwise you are combining 2 very different situations and the combined results are hard to interpret. To guard against this, it is often good to sample ranges of parameters in pieces, instead of one very wide range. And it's also always good to explore the outcomes in a plot."	"T4R2"	"Mean number of susceptible at end of simulation, increased birth"	"Rounded_Integer"	"Report the rounded integer"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	5	"Continue exploring by changing ranges for different parameters or initial conditions. It is likely that for some settings you will get warning or error messages. That generally means that the parameters for a given simulation are such that the differential equation solver can't properly run the model. That usually corresponds to biologically unrealistic parameter settings. Obvious silly choices would be ranges that include negative parameter values, but even some non-negative values of individual parameters or combinations of parameters can produce biologically unrealistic outcomes. Thus, when doing any kind of sampling, pay close attention to any warning or error messages from your code, and give your input ranges and results careful attention to make sure nothing unreasonable os happening."	"T5R1"	"Nothing"	"None"	""
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	6	"So far, we have changed parameter ranges and explored how that impacts the result. It is often useful to look at the impact of specific parameters on the outcomes of interest in more detail. This is where sensitivity analysis comes in. We run the same simulations, but now instead of plotting outcomes as a boxplot, we produce scatterplots for outcomes as function of each varied parameter. Let's explore that. Set values back as in task 1. Switch the plot type from boxplot to scatterplot, run the simulation. Take a close look at the scatterplots to investigate the relation between different parameters and the various outcomes. To investigate specific parameters, chose them as the output for the scatterplot. Look at the text below the plots. For each parameter-output pair, the code computes a rank correlation coefficient. Numbers close to 0 mean there is essentially no correlation, close to 1 or -1 means a large positive or negative correlation. (One could compute p-values for these correlations, but they are somewhat meaningless since the values will get smaller the more samples you use, so you can basically produce any p-value you want). With more samples, the patterns of correlation are clearer in the plots. Try running the simulation with different sample sizes (50, 100 or even more) to see the impact.
Compare the scatterplots and correlation coefficients with the results you found above, as well as the equation for S at steady state we used above. You might find that there is a lot of scatter in the data, too much to see clear patterns. One could always increase sample size which should help detect patterns, but it takes longer to run. Another option is to restrict the variability to a subset of parameters, which we'll do next."	"T6R1"	"The correlation between infection peak and _b_, 100 samples, rngseed = 100"	"Numeric"	"Report rounded to 2 digits (hundredths)"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	6	"So far, we have changed parameter ranges and explored how that impacts the result. It is often useful to look at the impact of specific parameters on the outcomes of interest in more detail. This is where sensitivity analysis comes in. We run the same simulations, but now instead of plotting outcomes as a boxplot, we produce scatterplots for outcomes as function of each varied parameter. Let's explore that. Set values back as in task 1. Switch the plot type from boxplot to scatterplot, run the simulation. Take a close look at the scatterplots to investigate the relation between different parameters and the various outcomes. To investigate specific parameters, chose them as the output for the scatterplot. Look at the text below the plots. For each parameter-output pair, the code computes a rank correlation coefficient. Numbers close to 0 mean there is essentially no correlation, close to 1 or -1 means a large positive or negative correlation. (One could compute p-values for these correlations, but they are somewhat meaningless since the values will get smaller the more samples you use, so you can basically produce any p-value you want). With more samples, the patterns of correlation are clearer in the plots. Try running the simulation with different sample sizes (50, 100 or even more) to see the impact.
Compare the scatterplots and correlation coefficients with the results you found above, as well as the equation for S at steady state we used above. You might find that there is a lot of scatter in the data, too much to see clear patterns. One could always increase sample size which should help detect patterns, but it takes longer to run. Another option is to restrict the variability to a subset of parameters, which we'll do next."	"T6R2"	"The correlation between final number of susceptibles and _m_, 100 samples, rngseed = 100"	"Numeric"	"Report rounded to 2 digits (hundredths)"
"dsaide_usanalysis"	"usanalysis"	"Uncertainty and Sensitivity Analysis"	7	"Let's explore in more detail how different parameters impact results by making the system less 'noisy'. To do so, we can impose no variability for some parameters. For the following parameters, set **both** their lower and upper bound to the specified value: b = 0.01, m = 10, n = 0.1. Give _g_ a mean of 1 and variance of 0.1. Run the simulation with 100 samples, rngseed=100, and produce a scatterplot for _g_. You will see very smooth curves since no other parameters vary. Note the correlation coefficients being essentially -1 or 1.
Now increase the 'noise' by also allowing _m_ to vary between 0.1 and 0.5. You will see much 'noisier' plots and the correlation coefficients change. Use 100 samples and rngseed=100. Look at both scatterplots for _g_ and _n_. Note the different distributions for _g_ and _m_. The former has more points around its mean and less for lower/higher values, while values for _m_ are uniformly distributed along the x-axis. This comes from the underlying assumption about how the parameters are distributed, gamma-distribution versus uniform distribution."	"T7R1"	"Correlation between _g_ and the outbreak peak, with _m_ being varied."	"Numeric"	"Report rounded to 2 digits (hundredths)"
