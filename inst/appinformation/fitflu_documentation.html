<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">



<title>Flu Fit</title>










</head>

<body>




<h1 class="title toc-ignore">Flu Fit</h1>



<div id="shinytab1" class="section level2">
<h2>Overview</h2>
<p>This app illustrates how to fit a mechanistic dynamical model to data and how to use simulated data to evaluate if it is possible to fit a specific model.</p>
</div>
<div id="shinytab2" class="section level2">
<h2>The Model</h2>
<div id="data" class="section level3">
<h3>Data</h3>
<p>For this app, weekly mortality data from the 1918 influenza pandemic in New York City is used. The data comes from <span class="citation">(Mills, Robins, and Lipsitch 2004)</span>. You can read a bit more about the data by looking at its help file with <code>help('flu1918data')</code>.</p>
<p>Alternatively, the model itself can be used to generate artificial data and fit to this generated data.</p>
</div>
<div id="simulation-model" class="section level3">
<h3>Simulation Model</h3>
<p>The underlying model that is being fit is a version of the basic SIR model. Since the available data is mortality, we need to keep track of dead individuals in the model, too. This can be achieved by including an additional compartment and let a fraction of infected individuals move into the dead instead of the recovered compartment.</p>
<div class="figure">
<img src="../media/fitflumodel.png" alt="Flow diagram for an SIR model with deaths." />
<p class="caption">Flow diagram for an SIR model with deaths.</p>
</div>
<p>The equations for the model are given by</p>
<p><span class="math display">\[
\begin{aligned}
\dot S &amp; = -bSI \\
\dot I &amp; = bSI - gI \\
\dot R &amp; = (1-f)gI \\
\dot D &amp; = fgI
\end{aligned}
\]</span></p>
<p>Since the individuals in the <em>R</em> compartment are not tracked in the data and do not further influence the model dynamics, we implement the model without the <em>R</em> compartment, i.e., the simulation runs the equations.</p>
<p><span class="math display">\[
\begin{aligned}
\dot S &amp; = -bSI \\
\dot I &amp; = bSI - gI \\
\dot D &amp; = fgI
\end{aligned}
\]</span></p>
</div>
<div id="fitting-model" class="section level3">
<h3>Fitting Model</h3>
<p>The data is reported in new deaths per week per 100,000 individuals. Our model tracks <em>cumulative</em>, not <em>new</em> deaths. The easiest way to match the two is to sequentially add the weekly deaths in the data to compute cumulative deaths for each week. We can then fit that quantity directly to the model variable <em>D</em>. Adjustment for population size is also needed, which is done by dividing the reported death rate by 100,000 and multiplying with the population size. This is further discussed in the tasks.</p>
<p>The app fits the model by minimizing the sum of square difference (SSR) between model predictions for cumulative deaths and the cumulative number of reported deaths for all data points, i.e. <span class="math display">\[
SSR= \sum_t (D_t - D^{data}_t)^2
\]</span> where the sum runs over the times at which data was reported.</p>
<p>It is also possible to set the app to fit the difference between the logarithm of data and model, i.e. <span class="math display">\[
SSR= \sum_t (\log(D_t) - \log(D^{data}_t))^2
\]</span></p>
<p>The choice to fit the data or the log of the data depends on the biological setting. Sometimes one approach is more suitable than the other. In this case, both approaches might be considered reasonable.</p>
<p>The app reports the final SSR for the fit.</p>
<p>While minimizing the sum of square difference between data and model prediction is a very common approach, it is not the only one. A more flexible formulation of the problem is to define a likelihood function, which is a mathematical object that compares the difference between model and data and has its maximum for the model settings that most closely describe the data. Under certain assumptions, maximizing the likelihood and minimizing the sum of squares are the same problem. Further details on this are beyond the basic introduction we want to provide here. Interested readers are recommended to look further into this topic, e.g., by reading about (maximum) likelihood on Wikipedia.</p>
</div>
<div id="computer-routines-for-fitting" class="section level3">
<h3>Computer routines for fitting</h3>
<p>A computer routine does the minimization of the sum of squares. Many such routines, generally referred to as ‘optimizers’, exist. For simple problems, e.g., fitting a linear regression model to data, any of the standard routines work fine. For the kind of minimization problem we face here, which involves a differential equation, it often makes a difference what numerical optimizer routine one uses. R has several packages for that purpose. In this app, we make use of several of the optimizers provided in the <code>nloptr</code> package. We have found those to have generally good performance. Nevertheless, it is often important to try different numerical routines and different starting points to ensure results are consistent.</p>
</div>
</div>
<div id="shinytab3" class="section level2">
<h2>What to do</h2>
<p>Since the data is in weeks, we also run the model in units of weeks.</p>
<div id="task-1" class="section level3">
<h3>Task 1</h3>
<ul>
<li>Find out roughly how many people lived in New York City in 1918. Use that number as your initial starting value for the susceptibles.</li>
<li>Set infected individuals to 1, no initial dead.</li>
<li>The model parameters, <em>b</em>, <em>g</em>, and <em>f</em>, are being fit. We still need to provide starting values for the fitting routine.</li>
<li>Set the starting value for the infection rate to <em>b=1e-6</em>.</li>
<li>Assume the duration of the infectious period is a week, set the initial recovery rate accordingly.</li>
<li>Assume that one percent of individuals died.</li>
<li>For each fitted parameter, choose some lower and upper bounds. Note that if the lower bound is not lower/equal and the upper, not higher/equal than the parameter, you will get an error message when you try to run the model.</li>
<li>We ignore the parameter values for simulated data, for now, they can be any value. The noise parameter also does not matter for now. Set usesimdata to 0.</li>
<li>We’ll fit the data ‘as is’ (not the log-transformed data), so set logfit to 0.</li>
<li>Start with a 1 fitting step/iteration and solver type 1. Run the simulation. Since you only do a single iteration, nothing is optimized. We are just doing this so you can see the time-series produced with these starting conditions. The ‘best fit’ parameter values are the same you started with.</li>
<li>Also record the SSR so you can compare it with the value after the fit. To that end, we assume that the NYC population was 5E6. With that choice, your SSR should be 7.3E12. Since the data is in weeks, we also run the model in units of weeks.</li>
</ul>
</div>
<div id="task-2" class="section level3">
<h3>Task 2</h3>
<ul>
<li>Set the iterations to 50, and rerun the simulation. All of the other settings should be the same from task 1. Look at the results. The plot shows the final fit. The model-predicted virus curve will be closer to the data. Also, the SSR value should have gone down, indicating a better fit. Also printed below the figure are the values of the fitted parameters at the end of the fitting process.</li>
<li>Set the iterations to 100, and rerun the simulation. You should see some further improvement in SSR. That indicates the previous fit was not the ‘best’ fit. (The best fit is the one with the lowest possible SSR).</li>
</ul>
</div>
<div id="task-3" class="section level3">
<h3>Task 3</h3>
<ul>
<li>Repeat the fits with 1/50/100 iterations using solvers/optimizers “2” and “3” for fitting without changing any of the other settings. You will notice that they do not give the same results. The reason for that is that different solvers are improving fits at different rates. Unfortunately, there is not always a single best optimizer. It depends on the problem/system. Thus it’s usually good to try multiple optimizers.</li>
<li>Depending on the speed of your computer, you can try to increase the number of iterations and see what best fit (smallest SSR) you can achieve.</li>
</ul>
<p>Generally, with increasing iterations, the fits get better. A fitting step or iteration is essentially an ‘attempt’ by the underlying code to find the best possible model. Increasing the tries usually improves the fit. In practice, one should not specify a fixed number of iterations, it is done here, so things run reasonably fast. Instead, one should ask the solver to run as long as it takes until it can’t find a way to improve the fit (can’t further reduce the SSR). The technical expression for this is that the solver has converged to the solution. This can be done with the solver used here (<code>nloptr</code> R package), but it would take too long, so we implement a “hard stop” after the specified number of iterations.</p>
</div>
<div id="task-4" class="section level3">
<h3>Task 4</h3>
<p>Ideally, with enough iterations, all solvers should reach the best fit with the lowest possible SSR. In practice, that does not always happen. Often it depends on the starting conditions. Let’s explore whether starting values matter.</p>
<ul>
<li>Run the simulation using solver 3 and 200 iterations, all of the other settings should be the same from task 1. If everthing was set correctly you should get SSR=4.3E10. Also, note the best-fit parameters. You can check the setting by changing the iteration to 1 and reruning the simmulation. You should get an SSR=7.3E12 just as in task 1.</li>
<li>Now change the starting values to a 5 percent mortality fraction and half a week of infectiousness duration. Re-run the simulation at 1 and 200 iterations. You should find before/after SSR values of 4.7E12 and 1.8E11. This means the starting values were somewhat better, but it didn’t help much for the fitting, at least not for 200 steps.</li>
<li>By trying different starting values, solvers and number of iterations, get an idea of the influence of starting conditions on fitting performance and results.</li>
</ul>
<p>In general, picking good starting values is important. One can get them by trying an initial visual fit or by doing several short fits, and use the best fit values at the end as starting values for a new fit. Especially if you want to fit multiple parameters, optimizers can ‘get stuck’ and even with running them for a long time. They might not find the best fit. What can happen is that a solver found a local optimum. It found a good fit, and now as it varies parameters, each new fit is worse, so the solver “thinks” it found the best fit, even though there are better ones further away in parameter space. Many solvers - even so-called ‘global’ solvers - can get stuck. Unfortunately, we never know if the solution is real or if the solver is stuck in a local optimum. One way to figure this out is to try different solvers and different starting conditions, and let each one run for a long time. If all return the same answer, no matter what type of solver you use and where you start, it’s quite likely (though not guaranteed) that we found the overall best fit (lowest SSR).</p>
<p>This problem of ‘getting stuck’ is something that frequently happens when trying to fit ODE models, which is in contrast to fitting with more standard models (e.g., a linear regression model), where this is not a problem. The technical term for this is that a simple regression optimization is <em>convex</em> while fitting an ODE model is usually not. That’s why you don’t have to worry if you found the right solution if you use say the <code>lm</code> or <code>glm</code> functions for fitting in <code>R</code>, but you do need to be careful about that if you fit more complicated models such as ODE or similar models.</p>
</div>
<div id="task-5" class="section level3">
<h3>Task 5</h3>
<ul>
<li>You might have noticed that the model is unusually far away from the data during the first several weeks. One reason for that could be that we started with 1 infected person, but in reality, there were already a lot of infected patients, so the deaths began to accumulate faster. I could have chosen to write the model such that you could fit the initial infected. I didn’t since it would overcomplicate the example and likely leading to overfitting. We don’t have much data, and if we fit too many parameters, the model would likely overfit the data (more on that below). Instead, let’s try if we can get a better fit by manually adjusting the initial number of infected.</li>
<li>Set values back to those of task 1 (the ‘Reset Inputs’ button should do the trick). Do a quick run with a single iteration to make sure you get the SSR from task 1.</li>
<li>Now set the initial number of infected to 1000, rerun a single iteration. You’ll notice in the plot that the model predicted deaths moves closer to the data in the first few weeks.</li>
<li>Maybe surprisingly, the SSR does not shrink. Can you figure out why? Take another look at the SSR equation and think about the impact of the later (higher value) data points compared to the earlier ones for the total SSR.</li>
</ul>
</div>
<div id="task-6" class="section level3">
<h3>Task 6</h3>
<p>In the previous task, you learned that for the SSR computation, discrepancies between large data and model values - which themselves tend to be larger in magnitude - often dominate the SSR expression. As an extreme example, if you have 2 data points you fit, one at 1E10 and the model predicts 1.1E10, that’s a difference of 1E9. The second data point is 1E7, and the model predicts 1E6. This is in some sense a more significant discrepancy, but the difference is only 9E6, much lower than the 1E9 for the first data point.</p>
<p>One way to give data of different magnitude more comparable weights is by fitting the log of the data. Note that fitting the data or the log of the data are different problems, and the choice should be made based on scientific/biological rationale.</p>
<ul>
<li>Let’s take a look at fitting the log. Set everything as in task 1, set fitlog to 1, and do a single iteration. The plot should look the same as in task 1, but the SSR is now computed on the log of the data and the model and should be 40.72.</li>
<li>Now increase the number of initial infected to 1000 again. You’ll see the model predictions go closer to the initial data again, and the SSR now becomes lower.</li>
<li>Repeat some of the task 1-3 exercises, now fitting the log of the data. Compare and contrast how results do and do not change.</li>
</ul>
<p>Again, fitting on both the linear or log scale are reasonable approaches, and the choice should be made based on underlying biology/science. A good rule of thumb is that if the data spans several orders of magnitude, fitting on the log scale is probably the better option. You probably already realized that you can of course, not compare the SSR between the two fitting approaches. SSR values only make sense to compare once you determined the scale for fitting.</p>
</div>
<div id="task-7" class="section level3">
<h3>Task 7</h3>
<p>One consideration when fitting these kinds of mechanistic models to data is the balance between data availability and model complexity. The more and “richer” data one has available the more parameters one can estimate and therefore, the more detailed a model can be. If one tries to ‘ask too much’ from the data, it leads to the problem of overfitting - trying to estimate more parameters than can be robustly estimated for a given dataset. One way to safeguard against overfitting is by probing if the model can in principle recover estimates in a scenario where parameter values are known. To do so, we can use our model with specific parameter values and simulate data. We can then fit the model to this simulated data. If everything works, we expect that - ideally independent of the starting values for our solver - we end up with estimated best-fit parameter values that agree with the ones we used to simulate the artificial data. We’ll try this now with the app.</p>
<ul>
<li>Set everything as in task 1. Now set the parameter values <em>bsim</em>, <em>gsim</em>, and <em>fsim</em> to the same values as the values used for starting the fitting routine.</li>
<li>Set <em>usesimdata</em> to 1.</li>
<li>Run for 1 fitting step. You should now see that the data has changed. Instead of the real data, we now use simulated data. Since the parameter values for the simulated data and the starting values for the fitting routine are the same, the time-series is on top of the data, and the SSR is (up to rounding errors) 0.</li>
</ul>
</div>
<div id="task-8" class="section level3">
<h3>Task 8</h3>
<p>Let’s see if the fitting routine can recover parameters from a simulation if we start with different initial guesses.</p>
<ul>
<li>Choose as values for simulated data parameters <em>bsim=5E-7</em>, <em>gsim=0.5</em> and <em>fsim = 0.05</em>.</li>
<li>Everything else should be as before. Importantly, the starting values for the parameters <em>b</em> and <em>g</em> are now different than the values used for the simulation.</li>
<li>Fit to the simulated data, run for 1 fitting step. You’ll see the (simulated) data change again. The SSR should be 7.6E11 (assuming we fit on a linear scale).</li>
<li>If you now run the fitting for many iterations/steps, what do you expect the final fit values for the parameters and the SSR to be?</li>
<li>Test your expectation by running for 100+ fitting steps with the different solvers.</li>
</ul>
</div>
<div id="task-9" class="section level3">
<h3>Task 9</h3>
<p>Theory suggests that if we run enough iterations, we should obtain a best fit with an SSR close to 0 and best fit values that agree with those used to generate the artificial data. You might find that this does not happen for all 3 solvers within a reasonable number of iterations. But for instance solver 2 and 1000 iterations should get you pretty close to what you started with.</p>
<p>That indicates that you can potentially estimate these parameters with that kind of data, at least if there is no noise. This is the most basic test. If you can’t get the best fit values to be the same as the ones you used to make the data, it means you are trying to fit more parameters than your data allows, i.e., you engage in overfitting. At that point, you will have to either get more data or reduce your fitted parameters, either by fixing some parameters based on biological a priori knowledge or by reducing the number of parameters through model simplification.</p>
</div>
<div id="task-10" class="section level3">
<h3>Task 10</h3>
<ul>
<li>Play around with different values for the parameters used to generate artificial data, and different values for the starting conditions and see if you find scenarios where you might not be able to get the solver to converge to a solution that agrees with the one you started with.</li>
<li>Also explore what if any impact fitting the data on a linear versus log scale has.</li>
</ul>
</div>
<div id="task-11" class="section level3">
<h3>Task 11</h3>
<ul>
<li>To make things a bit more realistic and harder, one can also add noise on top of the simulated data. Try that by playing with the ‘noise added’ parameter and see how well you can recover the parameter values for the simulation. Start with a small amount (e.g., 0.01) and increase.</li>
</ul>
<p>Note that since you now change your data after you simulated it, you don’t expect the parameter values for the simulation and those you obtain from your best fit to be the same. However, if the noise is not too large, you expect them to be similar.</p>
<p>You will likely find that for certain combinations of simulated data, noise added, and specific starting conditions, you might not get estimates that are close to those you used. This suggests that even for this simple model with 3 parameters, estimating those 3 parameters based on the available data is not straightforward.</p>
</div>
<div id="task-12" class="section level3">
<h3>Task 12</h3>
<ul>
<li>Keep exploring. Fitting these kinds of models can be tricky at times, and you might find strange behavior in this app that you don’t expect. Try to get to the bottom of what might be going on. This is an open-ended exploration, so I can’t give you a “hint”. Just try different things, try to understand as much as possible of what you observe.</li>
</ul>
</div>
</div>
<div id="shinytab4" class="section level2">
<h2>Further Information</h2>
<ul>
<li>This app (and all others) are structured such that the Shiny part (the graphical interface you see and the server-side function that goes with it) calls an underlying R script (or several) which runs the simulation for the model of interest and returns the results.</li>
<li>For this app, the underlying function running the simulation is called <code>simulate_fit_flu</code>. You can call them directly, without going through the shiny app. Use the <code>help()</code> command for more information on how to use the functions directly. If you go that route, you need to use the results returned from this function and produce useful output (such as a plot) yourself.</li>
<li>You can also download all simulator functions and modify them for your own purposes. Of course, to modify these functions, you’ll need to do some coding.</li>
<li>For examples on using the simulators directly and how to modify them, read the package vignette by typing <code>vignette('DSAIDE')</code> into the R console.</li>
<li>A good source for fitting models in <code>R</code> is <span class="citation">(Bolker 2008)</span>. Note though that the focus is on ecological data and ODE-type models are not/barely discussed.</li>
<li>This book <span class="citation">(Hilborn and Mangel 1997)</span> has nice explanations of data fitting, model comparison, etc. but is more theoretical.</li>
<li>Lot’s of good online material exists on fitting/inference. Most of the material is explained in the context of static, non-mechanistic, statistical or machine learning models, but a lot of the principles apply equally to ODEs.</li>
<li>A discussion of overfitting (also called ‘identifiability problem’) for ODEs is <span class="citation">(Miao et al. 2011)</span>.</li>
<li>Advanced functionality to fit stochastic models can be found in the <code>pomp</code> package in R. (If you don’t know what stochastic models are, check out the stochastic apps in DSAIDE.)</li>
<li>The data for this study is saved in the data variable <code>flu1918data</code>, you can read more about it by looking at its help file entry <code>help(flu1918data)</code>. The publication from which the data comes is <span class="citation">(Mills, Robins, and Lipsitch 2004)</span>.</li>
</ul>
<div id="references" class="section level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bolker08">
<p>Bolker, Benjamin M. 2008. <em>Ecological Models and Data in R</em>. Princeton University Press.</p>
</div>
<div id="ref-hilborn97">
<p>Hilborn, Ray, and Marc Mangel. 1997. <em>The ecological detective : confronting models with data</em>. Monographs in Population Biology 28. Princeton, N.J.: Princeton University Press.</p>
</div>
<div id="ref-miao11a">
<p>Miao, Hongyu, Xiaohua Xia, Alan S. Perelson, and Hulin Wu. 2011. “On Identifiability of Nonlinear ODE Models and Applications in Viral Dynamics.” <em>SIAM Review</em> 53 (1): 3. <a href="https://doi.org/10.1137/090757009">https://doi.org/10.1137/090757009</a>.</p>
</div>
<div id="ref-mills04">
<p>Mills, Christina E., James M. Robins, and Marc Lipsitch. 2004. “Transmissibility of 1918 Pandemic Influenza.” <em>Nature</em> 432 (7019): 904–6. <a href="https://doi.org/10.1038/nature03063">https://doi.org/10.1038/nature03063</a>.</p>
</div>
</div>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
